{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf0789-0c86-49c2-99b8-d1bb87fd3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789d599-3634-49bd-a688-55218a6dbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sepsis_comorbidity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3947e40-258a-4416-aac0-54dcb969887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vitals = pd.read_csv('vitals_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8210a3-38d6-45d1-a8a1-899698fa3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.sort_values([\"stay_id\",\"hour\"]).reset_index(drop=True)\n",
    "\n",
    "# future = data[[\"stay_id\",\"hour\",\"sepsis3_bin\"]].copy()\n",
    "\n",
    "# f1 = future.rename(columns={\"sepsis3_bin\":\"s_h1\"}); f1[\"hour\"] = f1[\"hour\"] - 1\n",
    "# f2 = future.rename(columns={\"sepsis3_bin\":\"s_h2\"}); f2[\"hour\"] = f2[\"hour\"] - 2\n",
    "# f3 = future.rename(columns={\"sepsis3_bin\":\"s_h3\"}); f3[\"hour\"] = f3[\"hour\"] - 3\n",
    "# f4 = future.rename(columns={\"sepsis3_bin\":\"s_h4\"}); f4[\"hour\"] = f4[\"hour\"] - 4\n",
    "\n",
    "# lab = data.merge(f1, on=[\"stay_id\",\"hour\"], how=\"left\") \\\n",
    "#           .merge(f2, on=[\"stay_id\",\"hour\"], how=\"left\") \\\n",
    "#           .merge(f3, on=[\"stay_id\",\"hour\"], how=\"left\") \\\n",
    "#           .merge(f4, on=[\"stay_id\",\"hour\"], how=\"left\")\n",
    "\n",
    "# for c in [\"s_h1\",\"s_h2\",\"s_h3\",\"s_h4\"]:\n",
    "#     lab[c] = lab[c].fillna(0).astype(int)\n",
    "\n",
    "# future_any = (lab[\"s_h1\"] | lab[\"s_h2\"] | lab[\"s_h3\"] | lab[\"s_h4\"]).astype(int)\n",
    "\n",
    "# lab[\"y_next_1to4h\"] = ((lab[\"sepsis3_bin\"] == 0) & (future_any == 1)).astype(int)\n",
    "\n",
    "# # Drop leakage rows where already septic at time t\n",
    "# data_labeled = lab[lab[\"sepsis3_bin\"] == 0].copy()\n",
    "\n",
    "# print(\"y_next_1to4h distribution:\\n\", data_labeled[\"y_next_1to4h\"].value_counts(dropna=False))\n",
    "# print(\"data_labeled shape:\", data_labeled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb4694-653c-406a-8f3b-e8b237a9b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def sepsis_to_01(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"Robust conversion to 0/1 from bool, strings, ints.\"\"\"\n",
    "    s = x.copy()\n",
    "    if pd.api.types.is_bool_dtype(s):\n",
    "        return s.astype(int)\n",
    "    ss = s.astype(str).str.strip().str.lower()\n",
    "    mapping = {\"true\":1, \"false\":0, \"1\":1, \"0\":0, \"yes\":1, \"no\":0, \"t\":1, \"f\":0}\n",
    "    out = ss.map(mapping)\n",
    "    out_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out = out.fillna(out_num)\n",
    "    return out.fillna(0).astype(int)\n",
    "\n",
    "def split_stays_with_pos(pos_stays, neg_stays, test_size=0.2, seed=42):\n",
    "    \"\"\"Ensure both train and test contain positive stays if any exist.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_stays = np.array(list(pos_stays))\n",
    "    neg_stays = np.array(list(neg_stays))\n",
    "    rng.shuffle(pos_stays); rng.shuffle(neg_stays)\n",
    "\n",
    "    if len(pos_stays) == 0:\n",
    "        # no positives available\n",
    "        return set(neg_stays), set()\n",
    "\n",
    "    n_pos_test = max(1, int(len(pos_stays) * test_size))\n",
    "    n_neg_test = int(len(neg_stays) * test_size)\n",
    "\n",
    "    test_stays = np.concatenate([pos_stays[:n_pos_test], neg_stays[:n_neg_test]])\n",
    "    train_stays = np.concatenate([pos_stays[n_pos_test:], neg_stays[n_neg_test:]])\n",
    "    return set(train_stays), set(test_stays)\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Checks\n",
    "# ---------------------------\n",
    "required_vitals_cols = {\"subject_id\",\"hadm_id\",\"stay_id\",\"hour\",\"concept\",\"valuenum\",\"intime\"}\n",
    "missing_v = required_vitals_cols - set(df_vitals.columns)\n",
    "if missing_v:\n",
    "    raise ValueError(f\"df_vitals missing columns: {missing_v}\")\n",
    "\n",
    "required_df_cols = {\"subject_id\",\"hadm_id\",\"stay_id\",\"sepsis3\"}\n",
    "missing_d = required_df_cols - set(df.columns)\n",
    "if missing_d:\n",
    "    raise ValueError(f\"df missing columns: {missing_d}\")\n",
    "\n",
    "KEYS = [\"subject_id\",\"hadm_id\",\"stay_id\",\"hour\"]\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Clean vitals + build hourly skeleton (ALL stays)\n",
    "# ---------------------------\n",
    "df_v = df_vitals.copy()\n",
    "df_v[\"intime\"] = pd.to_datetime(df_v[\"intime\"], errors=\"coerce\")\n",
    "df_v[\"charttime\"] = pd.to_datetime(df_v[\"charttime\"], errors=\"coerce\") if \"charttime\" in df_v.columns else pd.NaT\n",
    "df_v[\"hour\"] = pd.to_numeric(df_v[\"hour\"], errors=\"coerce\")\n",
    "df_v = df_v.dropna(subset=[\"stay_id\",\"hour\",\"concept\",\"valuenum\",\"intime\"]).copy()\n",
    "df_v[\"hour\"] = df_v[\"hour\"].astype(int)\n",
    "\n",
    "skeleton = (\n",
    "    df_v.groupby(KEYS, as_index=False)\n",
    "    .size()\n",
    "    .drop(columns=[\"size\"])\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Wide vitals features\n",
    "# ---------------------------\n",
    "v_agg = (\n",
    "    df_v.groupby(KEYS + [\"concept\"])[\"valuenum\"]\n",
    "      .agg([\"mean\",\"min\",\"max\",\"std\",\"count\",\"median\"])\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "v_wide = v_agg.pivot_table(\n",
    "    index=KEYS,\n",
    "    columns=\"concept\",\n",
    "    values=[\"mean\",\"min\",\"max\",\"std\",\"count\",\"median\"]\n",
    ")\n",
    "v_wide.columns = [f\"{concept}__{stat}\" for stat, concept in v_wide.columns]\n",
    "v_wide = v_wide.reset_index()\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Merge df onto skeleton (stay-level info broadcast to all hours)\n",
    "# ---------------------------\n",
    "d = df.copy()\n",
    "for c in [\"sofa_time\",\"suspected_infection_time\",\"admittime\",\"dischtime\",\"antibiotic_time\",\"culture_time\"]:\n",
    "    if c in d.columns:\n",
    "        d[c] = pd.to_datetime(d[c], errors=\"coerce\")\n",
    "\n",
    "# stay-level merge (most robust; avoids your earlier 'hour' KeyError)\n",
    "base = skeleton.merge(d, on=[\"subject_id\",\"hadm_id\",\"stay_id\"], how=\"left\")\n",
    "\n",
    "# merge vitals features\n",
    "data = base.merge(v_wide, on=KEYS, how=\"left\")\n",
    "\n",
    "# Add intime per stay from vitals to define timestamps\n",
    "stay_intime = df_v.groupby(\"stay_id\")[\"intime\"].min()\n",
    "data[\"intime\"] = data[\"stay_id\"].map(stay_intime)\n",
    "data[\"intime\"] = pd.to_datetime(data[\"intime\"], errors=\"coerce\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Build label from onset timestamp (THIS FIXES THE \"all zeros\" issue)\n",
    "# ---------------------------\n",
    "data[\"sepsis3_bin\"] = sepsis_to_01(data[\"sepsis3\"])\n",
    "\n",
    "# pick onset column\n",
    "if \"sofa_time\" in data.columns and data[\"sofa_time\"].notna().any():\n",
    "    onset_col = \"sofa_time\"\n",
    "elif \"suspected_infection_time\" in data.columns and data[\"suspected_infection_time\"].notna().any():\n",
    "    onset_col = \"suspected_infection_time\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"No usable onset time column found. Need non-null sofa_time or suspected_infection_time \"\n",
    "        \"to label 'onset in next 1–4 hours'.\"\n",
    "    )\n",
    "\n",
    "data[\"onset_time\"] = pd.to_datetime(data[onset_col], errors=\"coerce\")\n",
    "\n",
    "# current time at each hour\n",
    "data[\"t0\"] = data[\"intime\"] + pd.to_timedelta(data[\"hour\"], unit=\"h\")\n",
    "\n",
    "# label: onset in (t0, t0+4h]\n",
    "data[\"y_next_1to4h\"] = (\n",
    "    (data[\"sepsis3_bin\"] == 1) &\n",
    "    (data[\"onset_time\"].notna()) &\n",
    "    (data[\"onset_time\"] > data[\"t0\"]) &\n",
    "    (data[\"onset_time\"] <= data[\"t0\"] + pd.Timedelta(hours=4))\n",
    ").astype(int)\n",
    "\n",
    "# OPTIONAL: drop rows at/after onset to avoid training on post-event info\n",
    "data_labeled = data[(data[\"onset_time\"].isna()) | (data[\"t0\"] < data[\"onset_time\"])].copy()\n",
    "\n",
    "print(\"Using onset column:\", onset_col)\n",
    "print(\"Label distribution:\", data_labeled[\"y_next_1to4h\"].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# sanity: positive stays count\n",
    "stay_label = data_labeled.groupby(\"stay_id\")[\"y_next_1to4h\"].max()\n",
    "pos_stays = set(stay_label[stay_label == 1].index)\n",
    "neg_stays = set(stay_label[stay_label == 0].index)\n",
    "print(\"Num positive stays:\", len(pos_stays))\n",
    "print(\"Num negative stays:\", len(neg_stays))\n",
    "\n",
    "if len(pos_stays) == 0:\n",
    "    raise ValueError(\n",
    "        \"Still 0 positive stays after labeling. That means onset_time never falls within 1–4h of any hour row.\\n\"\n",
    "        \"Check that your 'hour' is aligned to ICU intime and that onset_time is truly within the ICU window.\"\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Build features (drop IDs/times/labels) + train/test split by stay_id\n",
    "# ---------------------------\n",
    "drop_cols = set(KEYS + [\n",
    "    \"sepsis3\",\"sepsis3_bin\",\"y_next_1to4h\",\n",
    "    \"onset_time\",\"t0\",\"intime\",\n",
    "    \"suspected_infection_time\",\"sofa_time\",\n",
    "    \"admittime\",\"dischtime\",\"antibiotic_time\",\"culture_time\"\n",
    "])\n",
    "feature_cols = [c for c in data_labeled.columns if c not in drop_cols]\n",
    "\n",
    "X = data_labeled[feature_cols]\n",
    "y = data_labeled[\"y_next_1to4h\"].astype(int)\n",
    "groups = data_labeled[\"stay_id\"]\n",
    "\n",
    "# split stays ensuring positives in test\n",
    "train_stays, test_stays = split_stays_with_pos(pos_stays, neg_stays, test_size=0.2, seed=42)\n",
    "train_mask = data_labeled[\"stay_id\"].isin(train_stays)\n",
    "test_mask  = data_labeled[\"stay_id\"].isin(test_stays)\n",
    "\n",
    "X_train, X_test = X.loc[train_mask].copy(), X.loc[test_mask].copy()\n",
    "y_train, y_test = y.loc[train_mask].copy(), y.loc[test_mask].copy()\n",
    "\n",
    "print(\"y_train:\", y_train.value_counts().to_dict(), \" | y_test:\", y_test.value_counts().to_dict())\n",
    "\n",
    "# Drop all-NaN columns in train (fixes SimpleImputer warning)\n",
    "all_nan_cols = X_train.columns[X_train.isna().all()].tolist()\n",
    "if all_nan_cols:\n",
    "    print(\"Dropping all-NaN train cols:\", all_nan_cols)\n",
    "    X_train.drop(columns=all_nan_cols, inplace=True)\n",
    "    X_test.drop(columns=all_nan_cols, inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Train RandomForest + probabilities\n",
    "# ---------------------------\n",
    "model = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "proba = model.predict_proba(X_test)\n",
    "if proba.shape[1] < 2:\n",
    "    raise RuntimeError(\"Model trained on one class unexpectedly. Check y_train distribution above.\")\n",
    "proba_test = proba[:, 1]\n",
    "\n",
    "# metrics (only if y_test has both classes)\n",
    "if y_test.nunique() == 2:\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, proba_test))\n",
    "    print(\"PR-AUC:\", average_precision_score(y_test, proba_test))\n",
    "else:\n",
    "    print(\"⚠️ y_test has one class; AUC metrics not defined. y_test:\", y_test.value_counts().to_dict())\n",
    "\n",
    "data_labeled.loc[X_test.index, \"pred_proba_next_1to4h\"] = proba_test\n",
    "\n",
    "# show sample\n",
    "display_cols = [\"stay_id\",\"hour\",\"y_next_1to4h\",\"pred_proba_next_1to4h\", onset_col]\n",
    "print(data_labeled[display_cols].head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0985e-ed75-4e7b-b4c4-c31e4027d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proba_all = model.predict_proba(X)   # X is full feature matrix on data_labeled\n",
    "if proba_all.shape[1] < 2:\n",
    "    raise RuntimeError(\"Model trained on one class unexpectedly. Check y_train distribution above.\")\n",
    "data_labeled[\"pred_proba_next_1to4h\"] = proba_all[:, 1]\n",
    "\n",
    "proba_test = data_labeled.loc[X_test.index, \"pred_proba_next_1to4h\"].values\n",
    "\n",
    "if y_test.nunique() == 2:\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, proba_test))\n",
    "    print(\"PR-AUC:\", average_precision_score(y_test, proba_test))\n",
    "else:\n",
    "    print(\"⚠️ y_test has one class; AUC metrics not defined. y_test:\", y_test.value_counts().to_dict())\n",
    "\n",
    "display_cols = [\"stay_id\",\"hour\",\"y_next_1to4h\",\"pred_proba_next_1to4h\", onset_col]\n",
    "print(data_labeled[display_cols].head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa9e29-5ea4-44ac-93ed-9b30b63d5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "\n",
    "y_true = data_labeled.loc[X_test.index, \"y_next_1to4h\"].astype(int).values\n",
    "p_hat  = data_labeled.loc[X_test.index, \"pred_proba_next_1to4h\"].astype(float).values\n",
    "\n",
    "def eval_at_threshold(y_true, p_hat, thr=0.5):\n",
    "    y_pred = (p_hat >= thr).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Core\n",
    "    sensitivity_recall = tp / (tp + fn) if (tp + fn) else np.nan   # TPR / Recall\n",
    "    specificity        = tn / (tn + fp) if (tn + fp) else np.nan   # TNR\n",
    "    precision          = tp / (tp + fp) if (tp + fp) else np.nan   # PPV\n",
    "    npv                = tn / (tn + fn) if (tn + fn) else np.nan\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # \"Stuff\" people often want\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else np.nan                 # 1 - specificity\n",
    "    fnr = fn / (fn + tp) if (fn + tp) else np.nan                 # 1 - recall\n",
    "    tpr = sensitivity_recall\n",
    "\n",
    "    return {\n",
    "        \"threshold\": thr,\n",
    "        \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"precision (PPV)\": precision,\n",
    "        \"recall/sensitivity (TPR)\": sensitivity_recall,\n",
    "        \"specificity (TNR)\": specificity,\n",
    "        \"NPV\": npv,\n",
    "        \"F1\": f1,\n",
    "        \"FPR\": fpr,\n",
    "        \"FNR\": fnr\n",
    "    }\n",
    "\n",
    "# ---- Single threshold (default 0.5) ----\n",
    "metrics_05 = eval_at_threshold(y_true, p_hat, thr=0.5)\n",
    "print(pd.Series(metrics_05))\n",
    "\n",
    "# ---- Also report threshold-free metrics ----\n",
    "if len(np.unique(y_true)) == 2:\n",
    "    print(\"\\nThreshold-free metrics:\")\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_true, p_hat))\n",
    "    print(\"PR-AUC:\", average_precision_score(y_true, p_hat))\n",
    "else:\n",
    "    print(\"\\n⚠️ y_true has one class in this test split; AUC metrics not defined.\")\n",
    "\n",
    "# ---- Sweep thresholds to pick a good operating point ----\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "rows = [eval_at_threshold(y_true, p_hat, thr=t) for t in thresholds]\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "# Example: best F1 threshold\n",
    "best_f1_row = df_thr.iloc[df_thr[\"F1\"].idxmax()]\n",
    "print(\"\\nBest F1 operating point:\")\n",
    "print(best_f1_row[[\"threshold\",\"F1\",\"precision (PPV)\",\"recall/sensitivity (TPR)\",\"specificity (TNR)\",\"TP\",\"FP\",\"TN\",\"FN\"]])\n",
    "\n",
    "# Example: threshold with recall >= 0.80 and max specificity\n",
    "target_recall = 0.80\n",
    "eligible = df_thr[df_thr[\"recall/sensitivity (TPR)\"] >= target_recall].copy()\n",
    "if len(eligible) > 0:\n",
    "    best_spec = eligible.iloc[eligible[\"specificity (TNR)\"].idxmax()]\n",
    "    print(f\"\\nBest specificity with recall >= {target_recall}:\")\n",
    "    print(best_spec[[\"threshold\",\"specificity (TNR)\",\"recall/sensitivity (TPR)\",\"precision (PPV)\",\"F1\",\"TP\",\"FP\",\"TN\",\"FN\"]])\n",
    "else:\n",
    "    print(f\"\\nNo threshold achieved recall >= {target_recall}. Try lowering target_recall.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec49ec2-8850-4c4e-bfb3-86e767769ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "\n",
    "y_true = data_labeled.loc[X_test.index, \"y_next_1to4h\"].astype(int).values\n",
    "p_hat  = data_labeled.loc[X_test.index, \"pred_proba_next_1to4h\"].astype(float).values\n",
    "\n",
    "def eval_at_threshold(y_true, p_hat, thr):\n",
    "    y_pred = (p_hat >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) else np.nan   # recall / TPR\n",
    "    specificity = tn / (tn + fp) if (tn + fp) else np.nan   # TNR\n",
    "    precision   = tp / (tp + fp) if (tp + fp) else np.nan   # PPV\n",
    "    npv         = tn / (tn + fn) if (tn + fn) else np.nan\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) else np.nan\n",
    "    bal_acc = (sensitivity + specificity) / 2 if (np.isfinite(sensitivity) and np.isfinite(specificity)) else np.nan\n",
    "    f1 = (2 * precision * sensitivity / (precision + sensitivity)) if (precision + sensitivity) else np.nan\n",
    "\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else np.nan\n",
    "    fnr = fn / (fn + tp) if (fn + tp) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"threshold\": thr,\n",
    "        \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_acc\": bal_acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall/sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "        \"NPV\": npv,\n",
    "        \"F1\": f1,\n",
    "        \"FPR\": fpr,\n",
    "        \"FNR\": fnr\n",
    "    }\n",
    "\n",
    "summary = {}\n",
    "if len(np.unique(y_true)) == 2:\n",
    "    summary[\"ROC-AUC\"] = roc_auc_score(y_true, p_hat)\n",
    "    summary[\"PR-AUC\"]  = average_precision_score(y_true, p_hat)\n",
    "else:\n",
    "    summary[\"ROC-AUC\"] = np.nan\n",
    "    summary[\"PR-AUC\"]  = np.nan\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "print(\"=== Threshold-free summary (test set) ===\")\n",
    "display(summary_df.style.format(\"{:.4f}\"))\n",
    "\n",
    "# common thresholds\n",
    "threshold_list = [0.1, 0.2, 0.3, 0.5]\n",
    "rows = [eval_at_threshold(y_true, p_hat, t) for t in threshold_list]\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"=== Metrics at selected thresholds (test set) ===\")\n",
    "display(\n",
    "    metrics_df[[\n",
    "        \"threshold\",\"TP\",\"FP\",\"TN\",\"FN\",\n",
    "        \"precision\",\"recall/sensitivity\",\"specificity\",\"F1\",\n",
    "        \"accuracy\",\"balanced_acc\",\"NPV\",\"FPR\",\"FNR\"\n",
    "    ]].style\n",
    "      .format({\n",
    "          \"precision\":\"{:.3f}\",\n",
    "          \"recall/sensitivity\":\"{:.3f}\",\n",
    "          \"specificity\":\"{:.3f}\",\n",
    "          \"F1\":\"{:.3f}\",\n",
    "          \"accuracy\":\"{:.3f}\",\n",
    "          \"balanced_acc\":\"{:.3f}\",\n",
    "          \"NPV\":\"{:.3f}\",\n",
    "          \"FPR\":\"{:.3f}\",\n",
    "          \"FNR\":\"{:.3f}\",\n",
    "      })\n",
    ")\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "sweep = pd.DataFrame([eval_at_threshold(y_true, p_hat, t) for t in thresholds])\n",
    "\n",
    "# Best F1 threshold\n",
    "best_f1 = sweep.iloc[sweep[\"F1\"].idxmax()][[\n",
    "    \"threshold\",\"F1\",\"precision\",\"recall/sensitivity\",\"specificity\",\"TP\",\"FP\",\"TN\",\"FN\"\n",
    "]]\n",
    "\n",
    "# Best specificity with recall >= target\n",
    "target_recall = 0.80\n",
    "eligible = sweep[sweep[\"recall/sensitivity\"] >= target_recall]\n",
    "best_spec = None\n",
    "if len(eligible) > 0:\n",
    "    best_spec = eligible.iloc[eligible[\"specificity\"].idxmax()][[\n",
    "        \"threshold\",\"specificity\",\"recall/sensitivity\",\"precision\",\"F1\",\"TP\",\"FP\",\"TN\",\"FN\"\n",
    "    ]]\n",
    "\n",
    "print(\"=== Best operating points (test set) ===\")\n",
    "best_tbl = pd.DataFrame([best_f1])\n",
    "best_tbl[\"criterion\"] = \"Best F1\"\n",
    "best_tbl = best_tbl[[\"criterion\"] + [c for c in best_tbl.columns if c != \"criterion\"]]\n",
    "\n",
    "if best_spec is not None:\n",
    "    tmp = pd.DataFrame([best_spec])\n",
    "    tmp[\"criterion\"] = f\"Max specificity with recall ≥ {target_recall}\"\n",
    "    tmp = tmp[[\"criterion\"] + [c for c in tmp.columns if c != \"criterion\"]]\n",
    "    best_tbl = pd.concat([best_tbl, tmp], ignore_index=True)\n",
    "\n",
    "display(\n",
    "    best_tbl.style.format({\n",
    "        \"threshold\":\"{:.2f}\",\n",
    "        \"F1\":\"{:.3f}\",\n",
    "        \"precision\":\"{:.3f}\",\n",
    "        \"recall/sensitivity\":\"{:.3f}\",\n",
    "        \"specificity\":\"{:.3f}\",\n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d1465-cb87-45b7-b394-f71faddb77d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
